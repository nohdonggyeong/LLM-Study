{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# API를 이용한 공개 모델 사용하기\n",
        "- Groq, Cerebras, Sambanova 등의 추론 전문 클라우드를 이용하면 다양한 무료 오픈 모델을 무료 사용 가능\n",
        "- Groq( https://console.groq.com )을 통해 진행"
      ],
      "metadata": {
        "id": "vdgMILUytbR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Groq Cloud의 기본 문법은 OpenAI, LangChain과 호환됨\n",
        "- 참고. OpenAI 호환 https://console.groq.com/docs/openai  \n",
        "- Groq의 API는 langchain_groq를 통해 연결\n",
        "- Groq은 gemma, llama 등 사용 가능"
      ],
      "metadata": {
        "id": "jZ0Mrf7ptyWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_groq langchain_community pymupdf pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFPXxz2Rvhez",
        "outputId": "1f17c58a-d2cd-4e46-cc41-97841ea6eadd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.5.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.60)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.25.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.13.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.5.0-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading groq-0.25.0-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, pypdf, pymupdf, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, groq, dataclasses-json, langchain_groq, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 groq-0.25.0 httpx-sse-0.4.0 langchain_community-0.3.24 langchain_groq-0.3.2 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 pymupdf-1.26.0 pypdf-5.5.0 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import time"
      ],
      "metadata": {
        "id": "rDy7BRQ9voDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a561948-bd8f-4ca4-8f65-fd81c716d9c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "간단한 체인을 만들고 실행  \n",
        "https://console.groq.com/keys에서 키 생성"
      ],
      "metadata": {
        "id": "JYblciLbUFsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GROQ_API_KEY'] = '...'\n",
        "\n",
        "llama_3_3 = ChatGroq(\n",
        "    temperature=0.4,\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        ")"
      ],
      "metadata": {
        "id": "e4JMYJqHUEYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "테스트를 위해, groq의 페이지를 불러와 질문해보기"
      ],
      "metadata": {
        "id": "TyzC7vEoUz7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://wow.groq.com/why-groq/\"\n",
        "loader = WebBaseLoader(url)\n",
        "\n",
        "docs = loader.load()\n",
        "question = \"Why is the inference speed of Groq Engine's LLM so fast? Answer in Korean.\"\n",
        "\n",
        "system = \"Answer the question from given contexts. Answer in Korean.\"\n",
        "human = \"\"\"\n",
        "Context: {context}\n",
        "\n",
        "---\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate([(\"system\", system), (\"human\", human)])\n",
        "\n",
        "chain = prompt | llama_3_3 | StrOutputParser()\n",
        "start = time.time()\n",
        "\n",
        "result = chain.invoke(\n",
        "    {\"context\": docs[0].page_content,\n",
        "     \"question\": question}\n",
        ")\n",
        "\n",
        "print(result)\n",
        "\n",
        "end = time.time()\n",
        "elapsed_time = end -start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzjO2b2_Uykc",
        "outputId": "a8b8c447-9ebf-4117-ce65-a00b54025e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Groq Engine의 LLM(Large Language Model)은 Groq의 LPU(Language Processing Unit) 기술을 사용하기 때문에 추론 속도가 매우 빠릅니다. LPU는 기존의 GPU(Graphics Processing Unit)와 달리 AI 추론과 언어 처리를 위해 특별히 설계되었기 때문에, 기존의 GPU보다 훨씬 더 빠르고 효율적인 추론을 수행할 수 있습니다. 또한, Groq의 LPU는 에너지 효율性도 매우 높아 대규모로 배포할 때에도 높은 성능을 유지할 수 있습니다. 이러한 기술적 특징으로 인해 Groq Engine의 LLM은 매우 빠른 추론 속도를 제공할 수 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Llama 3.3 Model Card 요약하기\n",
        "example_URL = 'https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md'\n",
        "\n",
        "loader = WebBaseLoader(example_URL)\n",
        "docs = loader.load()\n",
        "\n",
        "summarize_prompt = ChatPromptTemplate([\n",
        "    ('system', '''아래 [Paper]를 한국어로 3줄로 요약하세요.\n",
        "                  논문의 독창성과 기여점을 강조하세요.\n",
        "                  줄 바꿈 처리해주세요.\n",
        "               '''),\n",
        "    ('user', '''[Paper]: {text}''')\n",
        "])\n",
        "\n",
        "print('Total:', len(docs[0].page_content), 'characters')\n",
        "\n",
        "summarize_chain = summarize_prompt | llama_3_3 | StrOutputParser()\n",
        "summary = summarize_chain.invoke(docs[0].page_content)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0HEJQ_mWCnK",
        "outputId": "129eb356-51ff-40b1-c5d4-60e46acf9d1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total: 5050 characters\n",
            "이 논문은 LLaMA 모델의 개발과 평가에 관한 것입니다. \n",
            "이 모델은 대규모 데이터셋을 통해 훈련되어 다양한 자연어 처리 태스크에서 우수한 성능을 보입니다. \n",
            "이 논문의 독창성과 기여점은 LLaMA 모델의 개발과 평가 방법론에 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF 파일 다운로드하여 방법 3가지로 요약하기"
      ],
      "metadata": {
        "id": "N2d8pO0dnDPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.error import URLError, HTTPError\n",
        "import urllib.request\n",
        "\n",
        "path = './paper.pdf'\n",
        "# PDF 경로를 입력받아, paper.pdf에 저장하는 코드\n",
        "# 에러 발생 시, 직접 다운로드할 것\n",
        "def download_PDF(url):\n",
        "  try:\n",
        "    urllib.request.urlretrieve(\n",
        "        url,\n",
        "        filename=path\n",
        "    )\n",
        "  except HTTPError as e:\n",
        "    print(f\"HTTP Error: {e.code} - {e.reason}\")\n",
        "  except URLError as e:\n",
        "    print(f\"URL Error: {e.reason}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occured: {e}\")\n",
        "\n",
        "# 예시 논문 링크\n",
        "paper_URL = \"https://arxiv.org/pdf/2410.05983\"\n",
        "download_PDF(paper_URL)"
      ],
      "metadata": {
        "id": "F3daoMtfmJ7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain의 PyMuPdfLoader를 이용하여 PDF 파일을 불러옴  \n",
        "load()의 결과는 Page별로 리스트에 저장되므로, 이를 하나의 Document로 결합"
      ],
      "metadata": {
        "id": "RgxqC501oo0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "loader = PyMuPDFLoader(path)\n",
        "# 페이지별로 저장\n",
        "pages = loader.load()\n",
        "\n",
        "print('전체 페이지 수: ', len(pages))\n",
        "\n",
        "# 코퍼스에 모두 결합\n",
        "corpus = Document(page_content='')\n",
        "for page in pages:\n",
        "  corpus.page_content += page.page_content + ' '\n",
        "len(corpus.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSTR-O0conh6",
        "outputId": "75f4446e-9f45-4f43-b016-56d9a089cd18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 페이지 수:  34\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97365"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarization"
      ],
      "metadata": {
        "id": "K7bayRCGDvfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM의 Abstractive Summarization은 일반적으로 3개의 방법을 사용함\n",
        "- Stuff: 전체 코퍼스를 하나의 프롬프트에 넣고 요약\n",
        "- Map-Reduece: 코퍼스를 청크로 분리하고, 각 청크의 요약을 생성한 뒤 최종 요약 생성\n",
        "- Refine: 코퍼스를 청크로 분리하고, 처음 청크부터 하나씩 추가하며 순차적으로 요약 업데이트"
      ],
      "metadata": {
        "id": "FqGHiGILD3hT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "gemma 2 모델을 이용해 요약 수행해볼 것\n",
        "- 한국어 좀 함\n",
        "- Fine tuning 가능"
      ],
      "metadata": {
        "id": "XU_hcV-oEHFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(\n",
        "    temperature=0.4,\n",
        "    model=\"gemma2-9b-it\",\n",
        ")"
      ],
      "metadata": {
        "id": "ECVIOCiXrEn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stuff"
      ],
      "metadata": {
        "id": "wMiWgtYkElEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_prompt = ChatPromptTemplate([\n",
        "    ('system',\n",
        "     '''Summarize the following [Paper] in Korean.\n",
        "     Emphasize the uniqueness ane contribution of the paper.\n",
        "     The answer should be in 4 Paragraphs and up to 8 sentences each.\n",
        "     Please write your answer in Korean.'''),\n",
        "    ('user', '''{text}''')\n",
        "])\n",
        "chain = summarize_prompt | llm | StrOutputParser()\n",
        "\n",
        "try:\n",
        "  answer = chain.invoke({'text': corpus.page_content})\n",
        "  answer\n",
        "except Exception as e:\n",
        "  # Groq 토큰 입력 제한 초과\n",
        "  print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AddfmXZNEkX_",
        "outputId": "8635bb6c-6f96-4985-ec42-f022bd033e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: Error code: 413 - {'error': {'message': 'Request too large for model `gemma2-9b-it` in organization `org_01jw1bjnc5efvbzreazvb643ge` service tier `on_demand` on tokens per minute (TPM): Limit 15000, Requested 24554, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Map-Reduce\n",
        "Map-Reduce는 전체를 분할하여 개별 요약문을 생성한 뒤, 이를 모아 다시 최종 요약을 생성합니다.  \n",
        "긴 Context를 처리하기 위해, 전체 코퍼스를 작은 단위로 쪼개 보겠습니다."
      ],
      "metadata": {
        "id": "tlHL_ytsFhMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "document_list = text_splitter.split_documents([corpus])\n",
        "print(\"청크 개수: \", len(document_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fs1zWJ_FfPs",
        "outputId": "3c529cf8-43f9-43c3-b7fe-1aa1ed698476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "청크 개수:  11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이후 Map-Reduce와 Refine을 사용할 수 있습니다."
      ],
      "metadata": {
        "id": "jI4CubFDGEbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Map-Reduce"
      ],
      "metadata": {
        "id": "gY_Tai35GJuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Map 과정: 각 문서에 대해 요약을 생성합니다.\n",
        "map_prompt = ChatPromptTemplate([\n",
        "    ('system', '''Summarize the following [Paper] in Korean.\n",
        "    Emphasize the uniqueness and contribution of the paper.\n",
        "    The answer should be in 1 Paragraph and up to 8 sentences.\n",
        "    Please write your answer in Korean.'''),\n",
        "    ('user', '''[Paper]: {text}''')\n",
        "])\n",
        "\n",
        "map_chain = map_prompt | llm | StrOutputParser()\n",
        "\n",
        "raw_summaries = []\n",
        "for i in tqdm(range(len(document_list))):\n",
        "  response = map_chain.invoke(document_list[i].page_content)\n",
        "  raw_summaries.append(response)\n",
        "  print('')\n",
        "  print('#', i)\n",
        "  print(response)\n",
        "  print('=====================')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deaD-hX1GDb7",
        "outputId": "b916a6f6-9e5f-4e1e-a29b-a3e8db2c5e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|▉         | 1/11 [00:00<00:07,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 0\n",
            "본 논문은 장문맥 LLM을 RAG(Retrieval-Augmented Generation) 시스템에 적용할 때 나타나는 새로운 과제들을 분석하고 해결책을 제시합니다. 기존 연구들은 장문맥 LLM이 더 많은 검색 결과를 활용하여 성능을 향상시킬 것이라고 가정했지만, 본 논문은 LLM의 성능이 검색 결과 개수 증가에 따라 증가하다가 감소하는 현상을 발견했습니다. 이는 검색 결과 중에서도 관련성이 낮거나 오히려 오답 정보(hard negatives)가 포함되면서 LLM의 생성 성능에 부정적인 영향을 미치기 때문임을 보여줍니다. 이러한 문제점을 해결하기 위해, 본 논문은 검색 결과 재정렬, 암묵적 강도 조정, 그리고 중간 추론을 통한 명시적 관련성 조정 등 세 가지 새로운 방법을 제안합니다. 이러한 방법들은 장문맥 LLM을 이용한 RAG 시스템의 성능을 향상시키고, LLM이 검색 결과에서 중요한 정보를 효과적으로 추출하고 활용할 수 있도록 돕습니다. \n",
            "\n",
            "\n",
            "\n",
            "=====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 2/11 [00:01<00:06,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 1\n",
            "이 논문은 대규모 질의응답(RAG) 시스템에서 장문 맥락 LLM의 잠재력을 탐구하며, 기존 연구가 간과했던 핵심 문제점을 다룹니다. 특히, 다수의 검색된 문서가 존재하는 실제 RAG 환경에서 장문 맥락 LLM을 최적화하는 방법에 초점을 맞춥니다.  이 논문은 검색된 맥락의 크기가 RAG 성능에 미치는 영향과 검색 품질과 LLM 능력 간의 상호 작용을 분석하여 장문 맥락 LLM을 RAG에 적용하는 데 있어서 발생하는 새로운 과제들을 제시합니다. 특히, 강력한 검색기의 경우 높은 검색률에도 불구하고 추가된 문서가 성능 저하를 초래하는 현상을 발견하고, 이는 단순히 검색된 정보의 양을 넘어 검색된 정보의 품질과 LLM이 정보를 처리하는 능력이 중요함을 보여줍니다. 이러한 연구 결과는 장문 맥락 LLM을 RAG에 효과적으로 적용하기 위한 새로운 방향을 제시합니다.\n",
            "\n",
            "\n",
            "\n",
            "=====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 3/11 [00:02<00:05,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 2\n",
            "본 논문은 장 문맥 LLM의 RAG(Retrieval Augmented Generation) 성능에 미치는 \"hard negatives\"의 영향을 분석하고, 이를 해결하기 위한 새로운 방법론을 제시합니다. \n",
            "\n",
            "전통적인 평가 지표인 정확도만으로는 hard negatives가 RAG 성능에 미치는 영향을 충분히 반영하지 못한다는 점을 강조하며,  다양한 retriever를 사용한 실험을 통해 hard negatives의 강도가 retriever의 성능과 관련되어 있다는 것을 보여줍니다. 특히, 강력한 retriever에서 추출된 hard negatives는 약한 retriever에서 추출된 것보다 LLM에 더 큰 부정적인 영향을 미치는 것을 확인했습니다. 이를 해결하기 위해, 논문은 \"retrieval reordering\"이라는 새로운 방법을 제안합니다. 이 방법은 retrieved passages를 retriever의 relevance score 기반으로 재정렬하여 LLM이 가장 관련 있는 정보에 집중하도록 유도합니다. 실험 결과, retrieval reordering은 특히 많은 수의 retrieved passages가 사용될 때 RAG 성능을 향상시키는 효과를 보였습니다.  \n",
            "\n",
            "본 논문은 단순한 hard negatives의 존재를 넘어, 그 영향을 완전히 이해하고 해결하기 위한 새로운 접근 방식을 제시함으로써, 장 문맥 LLM의 RAG 성능 향상에 중요한 기여를 합니다. \n",
            "\n",
            "\n",
            "\n",
            "=====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▋      | 4/11 [00:02<00:05,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 3\n",
            "이 논문은 Retrieval Augmented Generation(RAG)에 적합한 대규모 언어 모델(LLM)을 위한 데이터 증강 미세 조정 방법을 제안합니다. 기존 연구는 LLM을 RAG에 적용할 때, 답변 생성 뿐만 아니라 관련된 텍스트를 효과적으로 파악하고 활용하는 능력이 부족하다는 점을 지적합니다. 이 논문에서는 두 가지 주요 접근 방식을 제안합니다. 첫째, LLM을 RAG-specific fine-tuning을 통해 훈련시켜 다양한 검색된 텍스트 컨텍스트에 노출시켜 관련 정보를 효과적으로 식별하고 활용할 수 있도록 합니다. 둘째, 훈련 과정에 중간 단계로 논리적 추론을 추가하여 LLM이 검색된 텍스트에서 중요한 정보를 명확하게 파악하고 분석하도록 돕습니다. 실험 결과, 제안된 방법은 다양한 데이터셋에서 기존 방법보다 우수한 성능을 보이며, 특히 많은 검색된 텍스트가 포함된 경우 더욱 효과적임을 보여줍니다. 이는 LLM의 RAG 능력을 향상시키기 위한 데이터 기반 접근 방식의 중요성을 강조합니다. \n",
            "\n",
            "\n",
            "\n",
            "=====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 5/11 [00:03<00:04,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 4\n",
            "본 논문은 긴 텍스트 컨텍스트를 가진 대규모 언어 모델(LLM)의 질의응답(RAG) 시스템에서 검색된 문서 수에 따른 성능 변화를 분석하고 이를 개선하기 위한 방법을 제시합니다. 기존 연구와 달리, 단순히 더 많은 문서를 사용하는 것이 항상 성능 향상을 가져오지 않는다는 것을 보여주며, \"hard negatives\"로 인해 성능이 저하될 수 있음을 발견했습니다. 이 문제를 해결하기 위해, 검색된 문서 순서 재배열, RAG 특화된 암묵적 LLM fine-tuning, 그리고 중간 추론을 포함하는 RAG 지향적인 LLM fine-tuning 등 세 가지 방법을 제안하고 평가했습니다. 특히, 데이터 분포, 훈련에 사용되는 검색 알고리즘, 훈련 컨텍스트 길이 등의 요소들이 LLM 성능에 미치는 영향을 체계적으로 분석하여 RAG 시스템 설계 및 훈련에 유용한 통찰력을 제공합니다. \n",
            "\n",
            "\n",
            "\n",
            "=====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▍    | 6/11 [00:14<00:20,  4.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 5\n",
            "이 논문은 장 문맥을 가진 대규모 언어 모델(LLM)이 Retrieval-Augmented Generation(RAG)에서 어떻게 성능을 보이는지 탐구합니다. 특히, 기존의 RAG 시스템에서 사용되는 단순한 검색기와 장 문맥 LLM의 조합이 긴 입력에 대해 어떻게 효과적인지 분석합니다.  논문은 다양한 LLM과 검색기, 그리고 긴 문맥 LLM에 대한 새로운 평가 방법을 통해 이러한 문제를 다룹니다.  결과적으로, 장 문맥 LLM이 긴 입력에 대한 RAG에서 유망한 성능을 보이며, 특히 강력한 검색기를 사용할 때 더욱 뛰어난 성능을 보인다는 것을 보여줍니다. 또한, 긴 문맥 LLM의 평가를 위해 \"hard negative\"를 사용하는 것이 중요하다는 점을 강조합니다. \n",
            "\n",
            "\n",
            "\n",
            "=====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▎   | 7/11 [00:29<00:30,  7.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 6\n",
            "본 논문은 장문 텍스트 입력에 대한 RAG (Retrieval Augmented Generation) 시스템에서 긴 문맥 LLM의 성능을 향상시키는 방법을 제시합니다. 특히, 다양한 검색기(retriever)가 제공하는 부정적 샘플(negative samples)의 품질이 LLM의 성능에 미치는 영향을 분석하고, 이를 개선하기 위한 방법을 제안합니다. 논문은 단순한 무작위 샘플링이나 약한 검색기에서 얻은 부정적 샘플보다, 강력한 검색기에서 얻은 샘플이 LLM에게 더 큰 영향을 미친다는 것을 보여주며, 이러한 문제점을 해결하기 위해 특정 규칙을 기반으로 부정적 샘플을 생성하는 새로운 방법을 제안합니다. 이는 긴 문맥 LLM을 사용하는 RAG 시스템의 성능 향상에 기여할 수 있는 중요한 연구 결과입니다.\n",
            "\n",
            "\n",
            "\n",
            "=====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 8/11 [00:39<00:25,  8.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 7\n",
            "본 논문은 긴 입력에 대한 RAG(Retrieval Augmented Generation)의 과제를 해결하기 위해 긴 맥락 LLM(Long-Context LLM)을 사용하는 방법을 제안합니다. 특히, 훈련 데이터를 다양한 답변 유형(짧은 답변, 긴 답변, 사실 검증, 닫힌 답변)으로 구성하여 모델의 강인성을 향상시키고, 짝수번째와 홀수번째 인덱스의 문서를 번갈아 순서를 바꾸는 새로운 Retrieval Reordering 알고리즘을 통해 모델의 문맥 이해 능력을 높입니다. 또한, 다양한 테스트 데이터셋과 검색 코퍼스를 사용하여 다양한 RAG 작업에서의 성능을 평가하고, 기존 LLM과 비교하여 향상된 성능을 보여줍니다. \n",
            "\n",
            "\n",
            "\n",
            "=====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 9/11 [00:51<00:19,  9.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 8\n",
            "본 논문은 Long-Context LLM이 RAG(Retrieval Augmented Generation)에 적용될 때 발생하는 과제들을 해결하기 위해 Gemini-1.5-pro 모델을 사용하여 중간 추론을 생성하도록 지도하는 방법을 제안합니다. 특히, 다양한 질문 유형(NQ, Wizard of Wikipedia, FEVER, MMLU)에 대한 RAG 튜닝을 위해 중간 추론을 생성하는 프롬프트를 설계하고, 이를 통해 모델이 질문에 대한 답변을 생성하기 전에 관련 문서를 분석하고 추론 과정을 명확하게 제시하도록 유도합니다. 이와 같은 접근 방식은 모델의 설명력을 향상시키고 RAG 성능을 높이는 데 기여합니다. \n",
            "\n",
            "\n",
            "\n",
            "=====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████ | 10/11 [01:01<00:09,  9.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 9\n",
            "본 논문은 Gemma-2-9B와 같은 대규모 언어 모델(LLM)을 RAG(Retrieval Augmented Generation) 기법으로 fine-tuning하여 장문 입력에 대한 성능을 향상시키는 방법을 제시합니다. 특히, 논문은 \n",
            "\n",
            "* **RAG fine-tuning의 효과**를 다양한 데이터셋에서 검증하고, \n",
            "* **중간 추론 단계를 추가한 RAG fine-tuning**이 더욱 향상된 성능을 달성함을 보여줍니다.\n",
            "\n",
            "또한, Mistral-Nemo-12B 모델을 사용한 데이터 증강 RAG fine-tuning 결과를 제시하여 다양한 모델에 대한 연구 결과를 보여줍니다.  이는 LLM의 장문 입력 처리 능력을 향상시키는 데 중요한 기여를 하는 독창적인 접근 방식입니다. \n",
            "\n",
            "\n",
            "\n",
            "=====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:14<00:00,  6.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 10\n",
            "본 논문은 긴 입력에 대한 RAG(Retrieval Augmented Generation) 적용에서 긴 문맥을 가진 31개의 대규모 언어 모델(LLM)을 사용하여 새로운 방법론을 제시하고 그 효과를 검증합니다. 특히, Mistral-Nemo-12B 및 Gemini-1.0-Pro와 같은 대규모 모델을 사용하여 RAG-specific fine-tuning 방법을 적용하고 그 성능을 평가했습니다.  결과는 RAG-specific fine-tuning이 일반적인 fine-tuning보다 성능을 향상시키며, 특히 intermediate reasoning step을 포함한 RAG FT w. Int 방법이 가장 우수한 결과를 보여줍니다. 또한, RAG-specific training data의 크기가 성능에 긍정적인 영향을 미치며, 일반적인 SFT data와 RAG-specific data를 결합하여 LLM의 성능을 향상시킬 수 있음을 보여줍니다. \n",
            "\n",
            "\n",
            "\n",
            "=====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_summaries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fch0GtgHVaJ",
        "outputId": "a51c79b1-7f8e-4eb5-abce-7c6b952f220a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['본 논문은 장문맥 LLM을 RAG(Retrieval-Augmented Generation) 시스템에 적용할 때 나타나는 새로운 과제들을 분석하고 해결책을 제시합니다. 기존 연구들은 장문맥 LLM이 더 많은 검색 결과를 활용하여 성능을 향상시킬 것이라고 가정했지만, 본 논문은 LLM의 성능이 검색 결과 개수 증가에 따라 증가하다가 감소하는 현상을 발견했습니다. 이는 검색 결과 중에서도 관련성이 낮거나 오히려 오답 정보(hard negatives)가 포함되면서 LLM의 생성 성능에 부정적인 영향을 미치기 때문임을 보여줍니다. 이러한 문제점을 해결하기 위해, 본 논문은 검색 결과 재정렬, 암묵적 강도 조정, 그리고 중간 추론을 통한 명시적 관련성 조정 등 세 가지 새로운 방법을 제안합니다. 이러한 방법들은 장문맥 LLM을 이용한 RAG 시스템의 성능을 향상시키고, LLM이 검색 결과에서 중요한 정보를 효과적으로 추출하고 활용할 수 있도록 돕습니다. \\n\\n\\n',\n",
              " '이 논문은 대규모 질의응답(RAG) 시스템에서 장문 맥락 LLM의 잠재력을 탐구하며, 기존 연구가 간과했던 핵심 문제점을 다룹니다. 특히, 다수의 검색된 문서가 존재하는 실제 RAG 환경에서 장문 맥락 LLM을 최적화하는 방법에 초점을 맞춥니다.  이 논문은 검색된 맥락의 크기가 RAG 성능에 미치는 영향과 검색 품질과 LLM 능력 간의 상호 작용을 분석하여 장문 맥락 LLM을 RAG에 적용하는 데 있어서 발생하는 새로운 과제들을 제시합니다. 특히, 강력한 검색기의 경우 높은 검색률에도 불구하고 추가된 문서가 성능 저하를 초래하는 현상을 발견하고, 이는 단순히 검색된 정보의 양을 넘어 검색된 정보의 품질과 LLM이 정보를 처리하는 능력이 중요함을 보여줍니다. 이러한 연구 결과는 장문 맥락 LLM을 RAG에 효과적으로 적용하기 위한 새로운 방향을 제시합니다.\\n\\n\\n',\n",
              " '본 논문은 장 문맥 LLM의 RAG(Retrieval Augmented Generation) 성능에 미치는 \"hard negatives\"의 영향을 분석하고, 이를 해결하기 위한 새로운 방법론을 제시합니다. \\n\\n전통적인 평가 지표인 정확도만으로는 hard negatives가 RAG 성능에 미치는 영향을 충분히 반영하지 못한다는 점을 강조하며,  다양한 retriever를 사용한 실험을 통해 hard negatives의 강도가 retriever의 성능과 관련되어 있다는 것을 보여줍니다. 특히, 강력한 retriever에서 추출된 hard negatives는 약한 retriever에서 추출된 것보다 LLM에 더 큰 부정적인 영향을 미치는 것을 확인했습니다. 이를 해결하기 위해, 논문은 \"retrieval reordering\"이라는 새로운 방법을 제안합니다. 이 방법은 retrieved passages를 retriever의 relevance score 기반으로 재정렬하여 LLM이 가장 관련 있는 정보에 집중하도록 유도합니다. 실험 결과, retrieval reordering은 특히 많은 수의 retrieved passages가 사용될 때 RAG 성능을 향상시키는 효과를 보였습니다.  \\n\\n본 논문은 단순한 hard negatives의 존재를 넘어, 그 영향을 완전히 이해하고 해결하기 위한 새로운 접근 방식을 제시함으로써, 장 문맥 LLM의 RAG 성능 향상에 중요한 기여를 합니다. \\n\\n\\n',\n",
              " '이 논문은 Retrieval Augmented Generation(RAG)에 적합한 대규모 언어 모델(LLM)을 위한 데이터 증강 미세 조정 방법을 제안합니다. 기존 연구는 LLM을 RAG에 적용할 때, 답변 생성 뿐만 아니라 관련된 텍스트를 효과적으로 파악하고 활용하는 능력이 부족하다는 점을 지적합니다. 이 논문에서는 두 가지 주요 접근 방식을 제안합니다. 첫째, LLM을 RAG-specific fine-tuning을 통해 훈련시켜 다양한 검색된 텍스트 컨텍스트에 노출시켜 관련 정보를 효과적으로 식별하고 활용할 수 있도록 합니다. 둘째, 훈련 과정에 중간 단계로 논리적 추론을 추가하여 LLM이 검색된 텍스트에서 중요한 정보를 명확하게 파악하고 분석하도록 돕습니다. 실험 결과, 제안된 방법은 다양한 데이터셋에서 기존 방법보다 우수한 성능을 보이며, 특히 많은 검색된 텍스트가 포함된 경우 더욱 효과적임을 보여줍니다. 이는 LLM의 RAG 능력을 향상시키기 위한 데이터 기반 접근 방식의 중요성을 강조합니다. \\n\\n\\n',\n",
              " '본 논문은 긴 텍스트 컨텍스트를 가진 대규모 언어 모델(LLM)의 질의응답(RAG) 시스템에서 검색된 문서 수에 따른 성능 변화를 분석하고 이를 개선하기 위한 방법을 제시합니다. 기존 연구와 달리, 단순히 더 많은 문서를 사용하는 것이 항상 성능 향상을 가져오지 않는다는 것을 보여주며, \"hard negatives\"로 인해 성능이 저하될 수 있음을 발견했습니다. 이 문제를 해결하기 위해, 검색된 문서 순서 재배열, RAG 특화된 암묵적 LLM fine-tuning, 그리고 중간 추론을 포함하는 RAG 지향적인 LLM fine-tuning 등 세 가지 방법을 제안하고 평가했습니다. 특히, 데이터 분포, 훈련에 사용되는 검색 알고리즘, 훈련 컨텍스트 길이 등의 요소들이 LLM 성능에 미치는 영향을 체계적으로 분석하여 RAG 시스템 설계 및 훈련에 유용한 통찰력을 제공합니다. \\n\\n\\n',\n",
              " '이 논문은 장 문맥을 가진 대규모 언어 모델(LLM)이 Retrieval-Augmented Generation(RAG)에서 어떻게 성능을 보이는지 탐구합니다. 특히, 기존의 RAG 시스템에서 사용되는 단순한 검색기와 장 문맥 LLM의 조합이 긴 입력에 대해 어떻게 효과적인지 분석합니다.  논문은 다양한 LLM과 검색기, 그리고 긴 문맥 LLM에 대한 새로운 평가 방법을 통해 이러한 문제를 다룹니다.  결과적으로, 장 문맥 LLM이 긴 입력에 대한 RAG에서 유망한 성능을 보이며, 특히 강력한 검색기를 사용할 때 더욱 뛰어난 성능을 보인다는 것을 보여줍니다. 또한, 긴 문맥 LLM의 평가를 위해 \"hard negative\"를 사용하는 것이 중요하다는 점을 강조합니다. \\n\\n\\n',\n",
              " '본 논문은 장문 텍스트 입력에 대한 RAG (Retrieval Augmented Generation) 시스템에서 긴 문맥 LLM의 성능을 향상시키는 방법을 제시합니다. 특히, 다양한 검색기(retriever)가 제공하는 부정적 샘플(negative samples)의 품질이 LLM의 성능에 미치는 영향을 분석하고, 이를 개선하기 위한 방법을 제안합니다. 논문은 단순한 무작위 샘플링이나 약한 검색기에서 얻은 부정적 샘플보다, 강력한 검색기에서 얻은 샘플이 LLM에게 더 큰 영향을 미친다는 것을 보여주며, 이러한 문제점을 해결하기 위해 특정 규칙을 기반으로 부정적 샘플을 생성하는 새로운 방법을 제안합니다. 이는 긴 문맥 LLM을 사용하는 RAG 시스템의 성능 향상에 기여할 수 있는 중요한 연구 결과입니다.\\n\\n\\n',\n",
              " '본 논문은 긴 입력에 대한 RAG(Retrieval Augmented Generation)의 과제를 해결하기 위해 긴 맥락 LLM(Long-Context LLM)을 사용하는 방법을 제안합니다. 특히, 훈련 데이터를 다양한 답변 유형(짧은 답변, 긴 답변, 사실 검증, 닫힌 답변)으로 구성하여 모델의 강인성을 향상시키고, 짝수번째와 홀수번째 인덱스의 문서를 번갈아 순서를 바꾸는 새로운 Retrieval Reordering 알고리즘을 통해 모델의 문맥 이해 능력을 높입니다. 또한, 다양한 테스트 데이터셋과 검색 코퍼스를 사용하여 다양한 RAG 작업에서의 성능을 평가하고, 기존 LLM과 비교하여 향상된 성능을 보여줍니다. \\n\\n\\n',\n",
              " '본 논문은 Long-Context LLM이 RAG(Retrieval Augmented Generation)에 적용될 때 발생하는 과제들을 해결하기 위해 Gemini-1.5-pro 모델을 사용하여 중간 추론을 생성하도록 지도하는 방법을 제안합니다. 특히, 다양한 질문 유형(NQ, Wizard of Wikipedia, FEVER, MMLU)에 대한 RAG 튜닝을 위해 중간 추론을 생성하는 프롬프트를 설계하고, 이를 통해 모델이 질문에 대한 답변을 생성하기 전에 관련 문서를 분석하고 추론 과정을 명확하게 제시하도록 유도합니다. 이와 같은 접근 방식은 모델의 설명력을 향상시키고 RAG 성능을 높이는 데 기여합니다. \\n\\n\\n',\n",
              " '본 논문은 Gemma-2-9B와 같은 대규모 언어 모델(LLM)을 RAG(Retrieval Augmented Generation) 기법으로 fine-tuning하여 장문 입력에 대한 성능을 향상시키는 방법을 제시합니다. 특히, 논문은 \\n\\n* **RAG fine-tuning의 효과**를 다양한 데이터셋에서 검증하고, \\n* **중간 추론 단계를 추가한 RAG fine-tuning**이 더욱 향상된 성능을 달성함을 보여줍니다.\\n\\n또한, Mistral-Nemo-12B 모델을 사용한 데이터 증강 RAG fine-tuning 결과를 제시하여 다양한 모델에 대한 연구 결과를 보여줍니다.  이는 LLM의 장문 입력 처리 능력을 향상시키는 데 중요한 기여를 하는 독창적인 접근 방식입니다. \\n\\n\\n',\n",
              " '본 논문은 긴 입력에 대한 RAG(Retrieval Augmented Generation) 적용에서 긴 문맥을 가진 31개의 대규모 언어 모델(LLM)을 사용하여 새로운 방법론을 제시하고 그 효과를 검증합니다. 특히, Mistral-Nemo-12B 및 Gemini-1.0-Pro와 같은 대규모 모델을 사용하여 RAG-specific fine-tuning 방법을 적용하고 그 성능을 평가했습니다.  결과는 RAG-specific fine-tuning이 일반적인 fine-tuning보다 성능을 향상시키며, 특히 intermediate reasoning step을 포함한 RAG FT w. Int 방법이 가장 우수한 결과를 보여줍니다. 또한, RAG-specific training data의 크기가 성능에 긍정적인 영향을 미치며, 일반적인 SFT data와 RAG-specific data를 결합하여 LLM의 성능을 향상시킬 수 있음을 보여줍니다. \\n\\n\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce 과정: 각 문서의 요약을 하나로 합칩니다.\n",
        "reduce_prompt = ChatPromptTemplate([\n",
        "    ('system', '''\n",
        "    Generate a summary of the following text that includes the following elements:\n",
        "    The answer should be in 4 Paragraphs and up to 8 sentences each.\n",
        "    Please write your answer in Korean.'''),\n",
        "    ('user', '''{text}\n",
        "    ---\n",
        "    요약(In Korean):''')\n",
        "])\n",
        "\n",
        "reduce_chain = reduce_prompt | llm | StrOutputParser()\n",
        "summary = reduce_chain.invoke('\\n---\\n'.join(raw_summaries))\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETj03b0gHlBI",
        "outputId": "73031036-5736-4c9f-cbfa-15d1f06bb2ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "본 논문은 장문맥 LLM을 RAG(Retrieval-Augmented Generation) 시스템에 적용할 때 나타나는 새로운 과제들을 분석하고 해결책을 제시합니다. 기존 연구들은 장문맥 LLM이 더 많은 검색 결과를 활용하여 성능을 향상시킬 것이라고 가정했지만, 본 논문은 LLM의 성능이 검색 결과 개수 증가에 따라 증가하다가 감소하는 현상을 발견했습니다. 이는 검색 결과 중에서도 관련성이 낮거나 오히려 오답 정보(hard negatives)가 포함되면서 LLM의 생성 성능에 부정적인 영향을 미치기 때문임을 보여줍니다. \n",
            "\n",
            "본 논문에서는 이러한 문제점을 해결하기 위해 검색 결과 재정렬, 암묵적 강도 조정, 그리고 중간 추론을 통한 명시적 관련성 조정 등 세 가지 새로운 방법을 제안합니다. 검색 결과 재정렬은 LLM이 가장 관련 있는 정보에 집중하도록 유도하고, 암묵적 강도 조정은 LLM이 검색 결과의 중요도를 더 잘 파악하도록 돕습니다. 또한, 중간 추론을 통해 LLM이 검색 결과를 분석하고 이해하는 과정을 명확하게 하고, 더 정확한 답변을 생성하도록 유도합니다. \n",
            "\n",
            "이러한 방법들은 장문맥 LLM을 이용한 RAG 시스템의 성능을 향상시키고, LLM이 검색 결과에서 중요한 정보를 효과적으로 추출하고 활용할 수 있도록 돕습니다. \n",
            "\n",
            "본 논문은 장문맥 LLM과 RAG 시스템의 궁극적인 잠재력을 이끌어내기 위한 중요한 발걸음입니다. \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9B짜리가 70B llama보다 Map-Reduce로 요약 잘 하는 것 확인함. llama가 한국어를 못하기도 함"
      ],
      "metadata": {
        "id": "yWPKLHuDItGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Refine\n",
        "- Refine은 청크를 순서대로 참고하며, 매 시점 요약문을 작성\n",
        "- 요약문과 새로운 청크를 비교하여, 요약문을 업데이트"
      ],
      "metadata": {
        "id": "CKCcYZMrIixC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_summarize_prompt = ChatPromptTemplate([\n",
        "    ('system', '''Summarize the following [Paper] in Korean.\n",
        "    Emphasize the uniqueness and contribution of the paper.\n",
        "    The answer should be in 1 Paragraph and up to 8 sentences.\n",
        "    Please write your answer in Korean.'''),\n",
        "    ('user', '''[Paper]: {text}''')\n",
        "])\n",
        "\n",
        "X = first_summarize_prompt.format_messages(text=document_list[0])\n",
        "intermediate_summary = llm.invoke(X).content\n",
        "print(intermediate_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA5Lv0z0IgW8",
        "outputId": "ccebed12-aa8d-4bae-b452-ede01fac55aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "본 논문은 긴 텍스트 맥락을 처리할 수 있는 대규모 언어 모델(LLM)을 RAG(Retrieval-Augmented Generation) 시스템에 적용했을 때 발생하는 새로운 과제와 해결책을 제시합니다. 기존 연구들은 긴 맥락 LLM이 더 많은 검색 결과를 활용하여 성능을 향상시킬 것이라고 가정했지만, 본 논문은 긴 맥락 LLM의 경우 검색 결과 개수가 증가할수록 성능이 오히려 저하되는 현상을 발견했습니다. 이는 검색 결과에 포함된 'hard negatives' (관련성이 낮은 정보)가 LLM의 생성 과정을 방해하기 때문으로 분석되었습니다. 이러한 문제를 해결하기 위해, 논문에서는 검색 결과 순서 재정렬, 묵시적 강인성 fine-tuning, 중간 추론을 활용한 명시적 관련성 fine-tuning 등 세 가지 새로운 방법을 제안합니다. 이러한 방법들은 긴 맥락 LLM의 RAG 성능을 향상시키고, 긴 맥락 LLM을 활용한 RAG 시스템의 효율성을 높이는 데 기여합니다. \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Refine Prompt\n",
        "refine_prompt = ChatPromptTemplate([\n",
        "    ('system', ''' A summary of the current state of a research paper is provided.\n",
        "    Read it and compare it with the newly provided contet to enhance the summary in Korean.\n",
        "    Write the complete summary in at most 4 Paragraphs and up to 8 sentences each.\n",
        "    Please write your answer in Korean.'''),\n",
        "    ('user', '''Summary: {previous_summary}\n",
        "    ---\n",
        "    New Content: {new_text}''')\n",
        "])\n",
        "\n",
        "refine_chain = refine_prompt | llm | StrOutputParser()\n",
        "\n",
        "for i in tqdm(range(1, len(document_list))):\n",
        "  intermediate_summary = refine_chain.invoke(\n",
        "      {'previous_summary': intermediate_summary, 'new_text': document_list[i].page_content}\n",
        "  )\n",
        "\n",
        "  print('')\n",
        "  print(intermediate_summary)\n",
        "  print('==================')\n",
        "# 길이를 지정하지 않으면 오래 걸릴 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "063pwNymJYaw",
        "outputId": "369a327c-d4e1-4456-db3b-6dfd0de9c4fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:01<00:11,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "본 논문은 긴 텍스트 맥락을 처리할 수 있는 대규모 언어 모델(LLM)을 RAG(Retrieval-Augmented Generation) 시스템에 적용했을 때 발생하는 새로운 과제와 해결책을 제시합니다. 기존 연구들은 긴 맥락 LLM이 더 많은 검색 결과를 활용하여 성능을 향상시킬 것이라고 가정했지만, 본 논문은 긴 맥락 LLM의 경우 검색 결과 개수가 증가할수록 성능이 오히려 저하되는 현상을 발견했습니다. 이는 검색 결과에 포함된 'hard negatives' (관련성이 낮은 정보)가 LLM의 생성 과정을 방해하기 때문으로 분석되었습니다. \n",
            "\n",
            "본 논문은 이러한 문제를 해결하기 위해, 검색 결과 순서 재정렬, 묵시적 강인성 fine-tuning, 중간 추론을 활용한 명시적 관련성 fine-tuning 등 세 가지 새로운 방법을 제안합니다. 이러한 방법들은 긴 맥락 LLM의 RAG 성능을 향상시키고, 긴 맥락 LLM을 활용한 RAG 시스템의 효율성을 높이는 데 기여합니다. \n",
            "\n",
            "또한, 본 논문은 긴 맥락 LLM을 RAG에 적용할 때 발생하는 새로운 과제에 대해 심층적으로 분석합니다. 먼저, 검색 결과의 양과 RAG 성능의 관계를 조사하여, 많은 검색 결과가 항상 성능 향상을 가져오는 것은 아니라는 것을 보여줍니다. 오히려, 관련성이 낮은 정보가 포함될 경우 LLM의 성능이 저하될 수 있습니다. 이를 해결하기 위해, 검색 결과의 질, 특히 \"hard negatives\"에 대한 이해가 중요하다는 것을 강조합니다.\n",
            "\n",
            "둘째, LLM의 능력과 검색 결과의 질 사이의 상호작용을 분석합니다. 높은 정확도를 가진 검색 결과가 더 많은 \"hard negatives\"를 포함할 수 있고, 이는 LLM의 성능 저하로 이어질 수 있습니다. 따라서, LLM이 효과적으로 활용할 수 있는 검색 결과의 질을 향상시키는 것이 중요합니다. 본 논문은 이러한 분석을 통해 긴 맥락 LLM을 RAG에 효과적으로 적용하기 위한 새로운 방향을 제시합니다. \n",
            "\n",
            "\n",
            "\n",
            "==================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:02<00:10,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## 긴 텍스트 맥락 처리를 위한 LLM과 RAG 시스템의 새로운 과제와 해결책\n",
            "\n",
            "본 논문은 긴 텍스트 맥락을 처리하는 대규모 언어 모델(LLM)을 Retrieval-Augmented Generation(RAG) 시스템에 적용했을 때 발생하는 새로운 과제와 해결책을 제시합니다. 기존 연구들은 긴 맥락 LLM이 더 많은 검색 결과를 활용하면 성능이 향상된다고 가정했지만, 본 논문은 긴 맥락 LLM의 경우 검색 결과 개수가 증가할수록 성능이 오히려 저하되는 현상을 발견했습니다. 이는 검색 결과에 포함된 'hard negatives' (관련성이 낮은 정보)가 LLM의 생성 과정을 방해하기 때문으로 분석되었습니다.\n",
            "\n",
            "본 논문은 이러한 문제를 해결하기 위해, 검색 결과 순서 재정렬, 묵시적 강인성 fine-tuning, 중간 추론을 활용한 명시적 관련성 fine-tuning 등 세 가지 새로운 방법을 제안합니다. 이러한 방법들은 긴 맥락 LLM의 RAG 성능을 향상시키고, 긴 맥락 LLM을 활용한 RAG 시스템의 효율성을 높이는 데 기여합니다.\n",
            "\n",
            "또한, 본 논문은 긴 맥락 LLM을 RAG에 적용할 때 발생하는 새로운 과제에 대해 심층적으로 분석합니다. 먼저, 검색 결과의 양과 RAG 성능의 관계를 조사하여, 많은 검색 결과가 항상 성능 향상을 가져오는 것은 아니라는 것을 보여줍니다. 오히려, 관련성이 낮은 정보가 포함될 경우 LLM의 성능이 저하될 수 있습니다. 이를 해결하기 위해, 검색 결과의 질, 특히 \"hard negatives\"에 대한 이해가 중요하다는 것을 강조합니다.\n",
            "\n",
            "둘째, LLM의 능력과 검색 결과의 질 사이의 상호작용을 분석합니다. 높은 정확도를 가진 검색 결과가 더 많은 \"hard negatives\"를 포함할 수 있고, 이는 LLM의 성능 저하로 이어질 수 있습니다. 따라서, LLM이 효과적으로 활용할 수 있는 검색 결과의 질을 향상시키는 것이 중요합니다. 본 논문은 이러한 분석을 통해 긴 맥락 LLM을 RAG에 효과적으로 적용하기 위한 새로운 방향을 제시합니다.\n",
            "\n",
            "본 논문은 'hard negatives'의 영향을 최소화하는 새로운 방법을 제시하며, 긴 맥락 LLM을 RAG에 적용하는 데 있어서 중요한 고려 사항을 제시합니다. 특히, 검색 결과의 질뿐 아니라 LLM의 내재적인 특성과 검색 결과의 상호작용을 고려해야 함을 강조합니다.\n",
            "\n",
            "\n",
            "\n",
            "==================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:04<00:09,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## 긴 텍스트 맥락 처리를 위한 LLM과 RAG 시스템: 새로운 과제와 해결책\n",
            "\n",
            "본 논문은 대규모 언어 모델(LLM)을 Retrieval-Augmented Generation(RAG) 시스템에 적용할 때 발생하는 새로운 과제와 해결책을 제시합니다. 특히, 긴 텍스트 맥락 처리에 초점을 맞춰 LLM의 성능이 검색 결과 개수 증가와 함께 오히려 저하되는 현상을 발견했습니다. 이는 검색 결과에 포함된 'hard negatives' (관련성이 낮은 정보)가 LLM의 생성 과정을 방해하기 때문으로 분석되었습니다.\n",
            "\n",
            "본 논문에서는 이러한 문제를 해결하기 위해 세 가지 새로운 방법을 제안합니다. 첫째, 검색 결과 순서 재정렬을 통해 LLM이 관련성이 높은 정보를 더 쉽게 접근할 수 있도록 합니다. 둘째, LLM을 RAG 데이터셋으로 묵시적으로 강인성 fine-tuning을 통해 hard negatives에 대한 저항력을 향상시킵니다. 셋째, 중간 추론을 활용한 명시적 관련성 fine-tuning을 통해 LLM이 검색 결과에서 중요한 정보를 명확하게 파악하고 활용하도록 훈련합니다. \n",
            "\n",
            "이러한 방법들은 긴 맥락 LLM의 RAG 성능을 향상시키고, 긴 맥락 LLM을 활용한 RAG 시스템의 효율성을 높이는 데 기여합니다.  \n",
            "\n",
            "또한, 본 논문은 긴 텍스트 맥락 LLM을 RAG에 적용할 때 발생하는 새로운 과제에 대해 심층적으로 분석합니다. 먼저, 검색 결과의 양과 RAG 성능의 관계를 조사하여, 많은 검색 결과가 항상 성능 향상을 가져오는 것은 아니라는 것을 보여줍니다. 오히려, 관련성이 낮은 정보가 포함될 경우 LLM의 성능이 저하될 수 있습니다. 이를 해결하기 위해, 검색 결과의 질, 특히 \"hard negatives\"에 대한 이해가 중요하다는 것을 강조합니다.\n",
            "\n",
            "둘째, LLM의 능력과 검색 결과의 질 사이의 상호작용을 분석합니다. 높은 정확도를 가진 검색 결과가 더 많은 \"hard negatives\"를 포함할 수 있고, 이는 LLM의 성능 저하로 이어질 수 있습니다. 따라서, LLM이 효과적으로 활용할 수 있는 검색 결과의 질을 향상시키는 것이 중요합니다. 본 논문은 이러한 분석을 통해 긴 맥락 LLM을 RAG에 효과적으로 적용하기 위한 새로운 방향을 제시합니다.\n",
            "\n",
            "본 논문은 'hard negatives'의 영향을 최소화하는 새로운 방법을 제시하며, 긴 맥락 LLM을 RAG에 적용하는 데 있어 중요한 고려 사항을 제시합니다. 특히, 검색 결과의 질뿐 아니라 LLM의 내재적인 특성과 검색 결과의 상호작용을 고려해야 함을 강조합니다.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "==================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:05<00:08,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## 긴 텍스트 맥락 처리를 위한 LLM과 RAG 시스템: 새로운 과제와 해결책 (개선된 요약)\n",
            "\n",
            "본 논문은 대규모 언어 모델(LLM)을 Retrieval-Augmented Generation(RAG) 시스템에 적용할 때 발생하는 새로운 과제와 해결책을 제시합니다. 특히, 긴 텍스트 맥락 처리에 초점을 맞춰 LLM의 성능이 검색 결과 개수 증가와 함께 오히려 저하되는 현상을 발견했습니다. 이는 검색 결과에 포함된 'hard negatives' (관련성이 낮은 정보)가 LLM의 생성 과정을 방해하기 때문으로 분석되었습니다.\n",
            "\n",
            "본 논문에서는 이러한 문제를 해결하기 위해 세 가지 새로운 방법을 제안합니다. 첫째, 검색 결과 순서 재정렬을 통해 LLM이 관련성이 높은 정보를 더 쉽게 접근할 수 있도록 합니다. 둘째, LLM을 RAG 데이터셋으로 묵시적으로 강인성 fine-tuning을 통해 hard negatives에 대한 저항력을 향상시킵니다. 셋째, 중간 추론을 활용한 명시적 관련성 fine-tuning을 통해 LLM이 검색 결과에서 중요한 정보를 명확하게 파악하고 활용하도록 훈련합니다. \n",
            "\n",
            "이러한 방법들은 긴 맥락 LLM의 RAG 성능을 향상시키고, 긴 맥락 LLM을 활용한 RAG 시스템의 효율성을 높이는 데 기여합니다. \n",
            "\n",
            "또한, 본 논문은 긴 텍스트 맥락 LLM을 RAG에 적용할 때 발생하는 새로운 과제에 대해 심층적으로 분석합니다. 먼저, 검색 결과의 양과 RAG 성능의 관계를 조사하여, 많은 검색 결과가 항상 성능 향상을 가져오는 것은 아니라는 것을 보여줍니다. 오히려, 관련성이 낮은 정보가 포함될 경우 LLM의 성능이 저하될 수 있습니다. 이를 해결하기 위해, 검색 결과의 질, 특히 \"hard negatives\"에 대한 이해가 중요하다는 것을 강조합니다.\n",
            "\n",
            "둘째, LLM의 능력과 검색 결과의 질 사이의 상호작용을 분석합니다. 높은 정확도를 가진 검색 결과가 더 많은 \"hard negatives\"를 포함할 수 있고, 이는 LLM의 성능 저하로 이어질 수 있습니다. 따라서, LLM이 효과적으로 활용할 수 있는 검색 결과의 질을 향상시키는 것이 중요합니다. 본 논문은 이러한 분석을 통해 긴 맥락 LLM을 RAG에 효과적으로 적용하기 위한 새로운 방향을 제시합니다.\n",
            "\n",
            "본 논문은 'hard negatives'의 영향을 최소화하는 새로운 방법을 제시하며, 긴 맥락 LLM을 RAG에 적용하는 데 있어 중요한 고려 사항을 제시합니다. 특히, 검색 결과의 질뿐 아니라 LLM의 내재적인 특성과 검색 결과의 상호작용을 고려해야 함을 강조합니다.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "==================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:23<00:36,  7.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## 긴 텍스트 맥락 처리를 위한 LLM과 RAG 시스템: 새로운 과제와 해결책 (개선된 요약)\n",
            "\n",
            "본 논문은 대규모 언어 모델(LLM)을 Retrieval-Augmented Generation(RAG) 시스템에 적용할 때 발생하는 새로운 과제와 해결책을 제시합니다. 특히, 긴 텍스트 맥락 처리에 초점을 맞춰 LLM의 성능이 검색 결과 개수 증가와 함께 오히려 저하되는 현상을 발견했습니다. 이는 검색 결과에 포함된 'hard negatives' (관련성이 낮은 정보)가 LLM의 생성 과정을 방해하기 때문으로 분석되었습니다.\n",
            "\n",
            "본 논문에서는 이러한 문제를 해결하기 위해 세 가지 새로운 방법을 제안합니다. 첫째, 검색 결과 순서 재정렬을 통해 LLM이 관련성이 높은 정보에 더 쉽게 접근할 수 있도록 합니다. 둘째, LLM을 RAG 데이터셋으로 묵시적으로 강인성 fine-tuning을 통해 hard negatives에 대한 저항력을 향상시킵니다. 셋째, 중간 추론을 활용한 명시적 관련성 fine-tuning을 통해 LLM이 검색 결과에서 중요한 정보를 명확하게 파악하고 활용하도록 훈련합니다. \n",
            "\n",
            "이러한 방법들은 긴 맥락 LLM의 RAG 성능을 향상시키고, 긴 맥락 LLM을 활용한 RAG 시스템의 효율성을 높이는 데 기여합니다. 또한, 본 논문은 긴 텍스트 맥락 LLM을 RAG에 적용할 때 발생하는 새로운 과제에 대해 심층적으로 분석합니다. 먼저, 검색 결과의 양과 RAG 성능의 관계를 조사하여, 많은 검색 결과가 항상 성능 향상을 가져오는 것은 아니라는 것을 보여줍니다. 오히려, 관련성이 낮은 정보가 포함될 경우 LLM의 성능이 저하될 수 있습니다. 이를 해결하기 위해, 검색 결과의 질, 특히 \"hard negatives\"에 대한 이해가 중요하다는 것을 강조합니다.\n",
            "\n",
            "둘째, LLM의 능력과 검색 결과의 질 사이의 상호작용을 분석합니다. 높은 정확도를 가진 검색 결과가 더 많은 \"hard negatives\"를 포함할 수 있고, 이는 LLM의 성능 저하로 이어질 수 있습니다. 따라서, LLM이 효과적으로 활용할 수 있는 검색 결과의 질을 향상시키는 것이 중요합니다. 본 논문은 이러한 분석을 통해 긴 맥락 LLM을 RAG에 효과적으로 적용하기 위한 새로운 방향을 제시합니다.\n",
            "\n",
            "\n",
            "본 논문은 'hard negatives'의 영향을 최소화하는 새로운 방법을 제시하며, 긴 맥락 LLM을 RAG에 적용하는 데 중요한 고려 사항을 제시합니다. 특히, 검색 결과의 질뿐 아니라 LLM의 내재적인 특성과 검색 결과의 상호작용을 고려해야 함을 강조합니다. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "==================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [01:00<01:09, 17.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## 긴 텍스트 맥락 처리를 위한 LLM과 RAG 시스템: 새로운 과제와 해결책 (개선된 요약)\n",
            "\n",
            "본 논문은 대규모 언어 모델(LLM)을 Retrieval-Augmented Generation(RAG) 시스템에 적용할 때 발생하는 새로운 과제와 해결책을 제시합니다. 특히, 긴 텍스트 맥락 처리에 초점을 맞춰 LLM의 성능이 검색 결과 개수 증가와 함께 오히려 저하되는 현상을 발견했습니다. 이는 검색 결과에 포함된 'hard negatives' (관련성이 낮은 정보)가 LLM의 생성 과정을 방해하기 때문으로 분석되었습니다. \n",
            "\n",
            "논문에서는 이러한 문제를 해결하기 위해 세 가지 새로운 방법을 제안합니다. 첫째, 검색 결과 순서 재정렬을 통해 LLM이 관련성이 높은 정보에 더 쉽게 접근할 수 있도록 합니다. 둘째, LLM을 RAG 데이터셋으로 묵시적으로 강인성 fine-tuning을 통해 hard negatives에 대한 저항력을 향상시킵니다. 셋째, 중간 추론을 활용한 명시적 관련성 fine-tuning을 통해 LLM이 검색 결과에서 중요한 정보를 명확하게 파악하고 활용하도록 훈련합니다. \n",
            "\n",
            "또한, 본 논문은 긴 맥락 LLM을 RAG에 적용할 때 발생하는 새로운 과제에 대해 심층적으로 분석합니다. 검색 결과의 양과 RAG 성능의 관계를 조사하여, 많은 검색 결과가 항상 성능 향상을 가져오는 것은 아니라는 것을 보여줍니다. 오히려, 관련성이 낮은 정보가 포함될 경우 LLM의 성능이 저하될 수 있습니다. 이를 해결하기 위해, 검색 결과의 질, 특히 \"hard negatives\"에 대한 이해가 중요하다는 것을 강조합니다. \n",
            "\n",
            "특히, 새로운 맥락 정보를 바탕으로, 논문은 다양한 retriever (BM25, e5, random sampling)를 사용하여 hard negatives의 강도를 비교 분석합니다.  결과적으로, e5 retriever가 BM25 retriever보다, 그리고 BM25 retriever가 random sampling보다 더 많은 \"Related but Irrelevant\" hard negatives를 제공하는 것으로 나타났습니다. 이는 LLM이 hard negatives에 취약하다는 것을 보여주며, RAG 시스템에서 hard negatives를 효과적으로 관리하는 것이 중요함을 강조합니다. \n",
            "\n",
            "\n",
            "\n",
            "==================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [01:01<00:36, 12.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## 긴 텍스트 맥락 처리를 위한 LLM과 RAG 시스템: 새로운 과제와 해결책 (개선된 요약)\n",
            "\n",
            "본 논문은 대규모 언어 모델(LLM)을 Retrieval-Augmented Generation(RAG) 시스템에 적용할 때 발생하는 새로운 과제와 해결책을 제시합니다. 특히, 긴 텍스트 맥락 처리에 초점을 맞춰 LLM의 성능이 검색 결과 개수 증가와 함께 오히려 저하되는 현상을 발견했습니다. 이는 검색 결과에 포함된 'hard negatives' (관련성이 낮은 정보)가 LLM의 생성 과정을 방해하기 때문으로 분석되었습니다. \n",
            "\n",
            "논문에서는 이러한 문제를 해결하기 위해 세 가지 새로운 방법을 제안합니다. 첫째, 검색 결과 순서 재정렬을 통해 LLM이 관련성이 높은 정보에 더 쉽게 접근할 수 있도록 합니다. 둘째, LLM을 RAG 데이터셋으로 묵시적으로 강인성 fine-tuning을 통해 hard negatives에 대한 저항력을 향상시킵니다. 셋째, 중간 추론을 활용한 명시적 관련성 fine-tuning을 통해 LLM이 검색 결과에서 중요한 정보를 명확하게 파악하고 활용하도록 훈련합니다. \n",
            "\n",
            "또한, 논문은 긴 맥락 LLM을 RAG에 적용할 때 발생하는 새로운 과제에 대해 심층적으로 분석합니다. 검색 결과의 양과 RAG 성능의 관계를 조사하여, 많은 검색 결과가 항상 성능 향상을 가져오는 것은 아니라는 것을 보여줍니다. 오히려, 관련성이 낮은 정보가 포함될 경우 LLM의 성능이 저하될 수 있습니다. 이를 해결하기 위해, 검색 결과의 질, 특히 \"hard negatives\"에 대한 이해가 중요하다는 것을 강조합니다. \n",
            "\n",
            "특히, 새로운 맥락 정보를 바탕으로, 논문은 다양한 retriever (BM25, e5, random sampling)를 사용하여 hard negatives의 강도를 비교 분석합니다.  결과적으로, e5 retriever가 BM25 retriever보다, 그리고 BM25 retriever가 random sampling보다 더 많은 \"Related but Irrelevant\" hard negatives를 제공하는 것으로 나타났습니다. 이는 LLM이 hard negatives에 취약하다는 것을 보여주며, RAG 시스템에서 hard negatives를 효과적으로 관리하는 것이 중요함을 강조합니다. \n",
            "\n",
            "\n",
            "\n",
            "==================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [01:12<00:23, 11.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## 긴 텍스트 맥락 처리를 위한 LLM과 RAG 시스템: 새로운 과제와 해결책 (개선된 요약)\n",
            "\n",
            "본 논문은 대규모 언어 모델(LLM)을 Retrieval-Augmented Generation(RAG) 시스템에 적용할 때 발생하는 새로운 과제와 해결책을 제시합니다. 특히, 긴 텍스트 맥락 처리에 초점을 맞춰 LLM의 성능이 검색 결과 개수 증가와 함께 오히려 저하되는 현상을 발견했습니다. 이는 검색 결과에 포함된 'hard negatives' (관련성이 낮은 정보)가 LLM의 생성 과정을 방해하기 때문으로 분석되었습니다. \n",
            "\n",
            "논문에서는 이러한 문제를 해결하기 위해 세 가지 새로운 방법을 제안합니다. 첫째, 검색 결과 순서 재정렬을 통해 LLM이 관련성이 높은 정보에 더 쉽게 접근할 수 있도록 합니다. 둘째, LLM을 RAG 데이터셋으로 묵시적으로 강인성 fine-tuning을 통해 hard negatives에 대한 저항력을 향상시킵니다. 셋째, 중간 추론을 활용한 명시적 관련성 fine-tuning을 통해 LLM이 검색 결과에서 중요한 정보를 명확하게 파악하고 활용하도록 훈련합니다. \n",
            "\n",
            "또한, 논문은 긴 맥락 LLM을 RAG에 적용할 때 발생하는 새로운 과제에 대해 심층적으로 분석합니다. 검색 결과의 양과 RAG 성능의 관계를 조사하여, 많은 검색 결과가 항상 성능 향상을 가져오는 것은 아니라는 것을 보여줍니다. 오히려, 관련성이 낮은 정보가 포함될 경우 LLM의 성능이 저하될 수 있습니다. 이를 해결하기 위해, 검색 결과의 질, 특히 \"hard negatives\"에 대한 이해가 중요하다는 것을 강조합니다. \n",
            "\n",
            "특히, 새로운 맥락 정보를 바탕으로, 논문은 다양한 retriever (BM25, e5, random sampling)를 사용하여 hard negatives의 강도를 비교 분석합니다.  결과적으로, e5 retriever가 BM25 retriever보다, 그리고 BM25 retriever가 random sampling보다 더 많은 \"Related but Irrelevant\" hard negatives를 제공하는 것으로 나타났습니다. 이는 LLM이 hard negatives에 취약하다는 것을 보여주며, RAG 시스템에서 hard negatives를 효과적으로 관리하는 것이 중요함을 강조합니다. \n",
            "\n",
            "\n",
            "\n",
            "==================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [01:26<00:12, 12.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## 긴 텍스트 맥락 처리를 위한 LLM과 RAG 시스템: 새로운 과제와 해결책 (개선된 요약)\n",
            "\n",
            "본 논문은 대규모 언어 모델(LLM)을 Retrieval-Augmented Generation(RAG) 시스템에 적용할 때 발생하는 새로운 과제와 해결책을 제시합니다. 특히, 긴 텍스트 맥락 처리에 초점을 맞춰 LLM의 성능이 검색 결과 개수 증가와 함께 오히려 저하되는 현상을 발견했습니다. 이는 검색 결과에 포함된 'hard negatives' (관련성이 낮은 정보)가 LLM의 생성 과정을 방해하기 때문으로 분석되었습니다.\n",
            "\n",
            "논문에서는 이러한 문제를 해결하기 위해 세 가지 새로운 방법을 제안합니다. 첫째, 검색 결과 순서 재정렬을 통해 LLM이 관련성이 높은 정보에 더 쉽게 접근할 수 있도록 합니다. 둘째, LLM을 RAG 데이터셋으로 묵시적으로 강인성 fine-tuning을 통해 hard negatives에 대한 저항력을 향상시킵니다. 셋째, 중간 추론을 활용한 명시적 관련성 fine-tuning을 통해 LLM이 검색 결과에서 중요한 정보를 명확하게 파악하고 활용하도록 훈련합니다.\n",
            "\n",
            "또한, 논문은 긴 맥락 LLM을 RAG에 적용할 때 발생하는 새로운 과제에 대해 심층적으로 분석합니다. 검색 결과의 양과 RAG 성능의 관계를 조사하여, 많은 검색 결과가 항상 성능 향상을 가져오는 것은 아니라는 것을 보여줍니다. 오히려, 관련성이 낮은 정보가 포함될 경우 LLM의 성능이 저하될 수 있습니다. 이를 해결하기 위해, 검색 결과의 질, 특히 \"hard negatives\"에 대한 이해가 중요하다는 것을 강조합니다. \n",
            "\n",
            "특히, 새로운 맥락 정보를 바탕으로, 논문은 다양한 retriever (BM25, e5, random sampling)를 사용하여 hard negatives의 강도를 비교 분석합니다.  결과적으로, e5 retriever가 BM25 retriever보다, 그리고 BM25 retriever가 random sampling보다 더 많은 \"Related but Irrelevant\" hard negatives를 제공하는 것으로 나타났습니다. 이는 LLM이 hard negatives에 취약하다는 것을 보여주며, RAG 시스템에서 hard negatives를 효과적으로 관리하는 것이 중요함을 강조합니다.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "==================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:42<00:00, 10.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## 긴 텍스트 맥락 처리를 위한 LLM과 RAG 시스템: 새로운 과제와 해결책 (개선된 요약)\n",
            "\n",
            "본 논문은 대규모 언어 모델(LLM)을 Retrieval-Augmented Generation(RAG) 시스템에 적용할 때 발생하는 새로운 과제와 해결책을 제시합니다. 특히, 긴 텍스트 맥락 처리에 초점을 맞춰 LLM의 성능이 검색 결과 개수 증가와 함께 오히려 저하되는 현상을 발견했습니다. 이는 검색 결과에 포함된 'hard negatives' (관련성이 낮은 정보)가 LLM의 생성 과정을 방해하기 때문으로 분석되었습니다.\n",
            "\n",
            "논문에서는 이러한 문제를 해결하기 위해 세 가지 새로운 방법을 제안합니다. 첫째, 검색 결과 순서 재정렬을 통해 LLM이 관련성이 높은 정보에 더 쉽게 접근할 수 있도록 합니다. 둘째, LLM을 RAG 데이터셋으로 묵시적으로 강인성 fine-tuning을 통해 hard negatives에 대한 저항력을 향상시킵니다. 셋째, 중간 추론을 활용한 명시적 관련성 fine-tuning을 통해 LLM이 검색 결과에서 중요한 정보를 명확하게 파악하고 활용하도록 훈련합니다.\n",
            "\n",
            "또한, 새로운 맥락 정보를 바탕으로, 논문은 다양한 retriever (BM25, e5, random sampling)를 사용하여 hard negatives의 강도를 비교 분석합니다.  결과적으로, e5 retriever가 BM25 retriever보다, 그리고 BM25 retriever가 random sampling보다 더 많은 \"Related but Irrelevant\" hard negatives를 제공하는 것으로 나타났습니다. 이는 LLM이 hard negatives에 취약하다는 것을 보여주며, RAG 시스템에서 hard negatives를 효과적으로 관리하는 것이 중요함을 강조합니다. \n",
            "\n",
            "**추가적으로, 논문은 다양한 LLM 모델(Mistral-Nemo-12B, Gemini-1.0-Pro)과 RAG fine-tuning 방법을 사용하여 실험 결과를 제시합니다.**  Mistral-Nemo-12B 및 Gemini-1.0-Pro 모델을 사용한 실험 결과는 LLM을 RAG 데이터셋으로 묵시적으로 fine-tuning하는 것만으로도 성능 향상을 가져올 수 있음을 보여줍니다. 또한, 중간 추론을 활용한 명시적 RAG fine-tuning이 더욱 향상된 성능을 달성할 수 있음을 확인했습니다. \n",
            "\n",
            "**마지막으로, 논문에서는 RAG fine-tuning 데이터의 크기가 LLM 성능에 미치는 영향을 분석합니다.**  더 많은 RAG 데이터를 사용할수록 LLM의 RAG 성능이 향상됨을 확인했습니다. 이는 RAG fine-tuning에 대한 데이터 품질과 양이 중요함을 시사합니다.\n",
            "\n",
            "\n",
            "\n",
            "==================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "일반적으로, 긴 출력이나 최종 출력이 필요한 경우에는 고성능 모델, 짧은 출력에는 경량 모델이나 저렴한 모델 추천"
      ],
      "metadata": {
        "id": "aKFGwo7MK2Tn"
      }
    }
  ]
}