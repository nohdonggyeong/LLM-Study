{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LLM 생성 결과물의 특징\n",
        "- 다량의 사전 학습에 기반한 유연성과 활용성\n",
        "  - 자연어 처리에 대한 깊은 이해와 높은 수준의 생성\n",
        "- LLM의 답변은 학습 데이터에 전적으로 의존한다.\n",
        "  - 익숙하지 않은 데이터에 대한 질문은 정확하지 않을 수 있다. \"확률론적 앵무새\"\n",
        "  "
      ],
      "metadata": {
        "id": "OIr1FL-bh4Xz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG: 정확성을 높이는 방법\n",
        "- 검색 증강 생성(Retrieval-Augmented Generation)\n",
        "  - 데이터베이스에서 질의와 관련된 정보를 검색하여 이를 프롬프트에 함께 전달하는 방법 \"프롬프트를 증강시킴\"\n",
        "  - LLM의 가장 대표적인 어플리케이션"
      ],
      "metadata": {
        "id": "qihPk2rIiViQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG의 활용\n",
        "- 예시 프롬프트\n",
        "  - Context와 Question을 구분하여 제공\n",
        "  - 할루시네이션을 최소화하고, 모르는 주제에 대해서도 답변할 수 있음"
      ],
      "metadata": {
        "id": "8eeYdDBJjAC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG의 Retrieval: 검색\n",
        "- 정확한 Context를 제공하는 과정\n",
        "  - 검색이 제대로 되지 않는다면, 출력이 제대로 될 수 없음\n",
        "- 관련성을 어떻게 측정하는가?\n",
        "  - 텍스트 간의 유사도 측정 방법: 임베딩 or 키워드 기반 비교"
      ],
      "metadata": {
        "id": "v0N1_ufCjb0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG의 5 step process\n",
        "- Indexing\n",
        "  - 데이터베이스 구성\n",
        "- Processing\n",
        "  - 입력 쿼리 전처리\n",
        "- Searching\n",
        "  - 쿼리가 주어지면, 가장 적합한 데이터 검색\n",
        "- Argumenting\n",
        "  - 데이터와 쿼리를 이용해 프롬프트 증강\n",
        "- Generating\n",
        "  - LLM에 입력하여 답변 생성"
      ],
      "metadata": {
        "id": "p4sgWpfwFqk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing: 데이터 준비하기\n",
        "- 하나의 Context에 전체 문서를 입력하는 것은 효과적이지 않음\n",
        "  - 필요한 부분만 검색할 수 있도록 분리(Chunking)\n",
        "  - e.g. 페이지 단위로 데이터베이스에 저장\n",
        "- 일반적인 텍스트의 경우, 문자나 토큰 단위의 청킹 수행\n",
        "  - 청크 사이즈: 각 청크의 크기\n",
        "  - 최적의 청크 사이즈는 도메인/데이터마다 다를 수 있음"
      ],
      "metadata": {
        "id": "p8g2W1yCTdHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing: 적절한 청크 사이즈 선택하기\n",
        "- 청크 사이즈가 작은 경우\n",
        "  - 주변 정보 활용 어려움\n",
        "- 청크 사이즈가 큰 경우\n",
        "  - 불필요한 정보 포함\n",
        "  - 임베딩의 정확도 감소"
      ],
      "metadata": {
        "id": "KAkcaBA8UHWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing: 사용자 쿼리 처리하기\n",
        "- 검색이 잘 되도록 하기 위한 전처리\n",
        "  - RAG로 답변할 문제인지를 사전에 파악하기\n",
        "- 너무 짧은 쿼리는\n",
        "  - 의미 이해를 위한 정보가 불충분\n",
        "  - 이전 대화 내역을 고려하여 Contextualize\n",
        "- 너무 긴 쿼리는\n",
        "  - 불필요한 정보가 포함\n",
        "  - 요약하거나 일부 내용 삭제"
      ],
      "metadata": {
        "id": "PnUyFNZHU9nu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Searching: 쿼리와 적합한 데이터 검색하기\n",
        "- Semantic 검색과 Lexical 검색\n",
        "  - Embedding 기반의 Semantic 검색\n",
        "    - 정확하게 일치하지 않아도 유사한 의미를 탐색\n",
        "  - Keyword 기반의 Lexical 검색 (BM25, TF-IDF)\n",
        "    - 정확하게 일치하는 경우에는 높은 가중치\n",
        "    - 모델명, 법령 등 용어 검색에 유리\n",
        "  - Semantic 검색의 경우, 벡터 데이터베이스를 활용\n",
        "    - 최근 흐름으로 두 검색을 결합한 Hybrid Search"
      ],
      "metadata": {
        "id": "gynv2Ks_Vhy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Searching: 벡터 데이터베이스는 임베딩 저장 공간\n",
        "- 벡터 형태의 데이터 저장 및 검색에 최적화된 소프트웨어\n",
        "  - 비정형 데이터를 임베딩으로 변환하고, 이를 저장 및 검색\n",
        "  - 트랜스포머 기반 기존 Text Embedding 기술 활용\n",
        "- 자연어, 그래프, 이미지 등의 데이터가 많아지면서 중요도 증가\n",
        "  - LLM뿐만 아니라 데이터베이스 자체로도 활용"
      ],
      "metadata": {
        "id": "78uktqr0W2pH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Searching: 벡터 DB 활용 예시\n",
        "- 검색: 질문(Query)과 청크(Chunk) 비교\n",
        "  - 질문의 임베딩과 청크들의 임베딩 유사도 계산\n",
        "  - Top K Search 후 Return\n",
        "- 청크 검색 후\n",
        "  - Task에 따른 활용\n",
        "    - RAG: LLM 프롬프트에 추가하여 답변 생성\n",
        "    - Recommendation: 해당 상품 혹은 문서 전달"
      ],
      "metadata": {
        "id": "Ar6JczKqYq3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Searching: 대표적 Vector Database\n",
        "- Pinecone: 클라우드 기반의 유료 서비스\n",
        "- Milvus, Qdrant, Chroma, Weaviate: 무료 사용 지원\n",
        "- Popularity 순위\n",
        "  - https://db-engines.com/en/ranking/vector+dbms\n",
        "  - Elasticsearch와 같이 Vector 지원되는 Engine 사용해도 됨\n",
        "- Vector Database의 Metric Types\n",
        "  - Cosine Distance\n",
        "    - 1-Cosine Similarity\n",
        "  - Euclidean Distance (L2 Distance)\n",
        "    - 벡터 간의 직선 거리\n",
        "  - MMR\n",
        "    - 다양성을 고려한 방법\n",
        "    - Alpha * 쿼리와의 유사성 - (1 - Alpha) * (Context 중 가장 가까운 청크와의 유사성) 순으로 검색"
      ],
      "metadata": {
        "id": "KWATK2Z5Zjhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face 공개 모델과 공개 임베딩을 이용한 RAG 실습\n",
        "- RAG는 Retrieval-Augmented Generation의 약자\n",
        "- 질문이 주어지면 관련 있는 문서를 찾아 프롬프트에 추가하는 방식의 어플리케이션\n",
        "- RAG의 과정\n",
        "1. Indexing: 문서를 받아 검색이 잘 되도록 저장\n",
        "2. Processing: 입력 쿼리를 전처리하여 검색에 적절한 형태로 변환\n",
        "3. Search(Retrieval): 질문이 주어진 상황에서 가장 필요한 참고자료를 검색\n",
        "4. Augmenting: Retrieval의 결과와 입력 프롬프트를 이용해 LLM에 전달할 프롬프트를 생성\n",
        "5. Generation: LLM이 출력을 생성\n",
        "  \n",
        "오른쪽 위 ▼ 화살표 클릭 --> 런타임 유형 변경 --> T4 GPU 설정  \n",
        "LLM 모델은 Alibaba Cloud의 Qwen2.5-7b-instruct  \n",
        "Embedding 모델은 Microsoft의 Multilingual-E5-small"
      ],
      "metadata": {
        "id": "JZbSp_lWFrYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf sentence_transformers wikipedia langchain langchain-community langchain_huggingface bitsandbytes accelerate huggingface_hub langchain_chroma"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9kbsll3OYp_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 불러오기"
      ],
      "metadata": {
        "id": "gu9OgdXZhscX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import transformers\n",
        "\n",
        "model_id = 'Qwen/Qwen2.5-7B-Instruct'\n",
        "\n",
        "# 양자화\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bits=True,  # 4비트 양화\n",
        "    bnb_4bit_use_double_quant=True,  # 이중 양자화\n",
        "    bnb_4bit_quant_type='nf4',  # 분위수 기반 양자화\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16  # 원 모델의 가중치가 bf16임을 표시\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype='auto',\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0},\n",
        ")"
      ],
      "metadata": {
        "id": "-xbieCTJhrp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "gen_config = dict(\n",
        "    do_sample=True,  # 확률 기반의 샘플링 표시\n",
        "    max_new_tokens=512,  # 최대 512 토큰 출력\n",
        "    repetition_penalty=1.05,  # 반복 페널티\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,  # 누적확률 기반 상위 0.8 이내 토큰만 샘플링\n",
        "    top_k=20,  # Top 20 토큰만 샘플링\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,  # 입력을 함께 출력할지 여부\n",
        "    **gen_config,\n",
        ")"
      ],
      "metadata": {
        "id": "rGGfFRMyi6kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "랭체인과 파이프라인 연결하기"
      ],
      "metadata": {
        "id": "NMZzfOxTxKFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "chat_model = ChatHuggingFace(llm=llm, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "LhUn3ShxoR0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG Prompt 구성하기"
      ],
      "metadata": {
        "id": "4IoZRneVxmjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 성능 향상을 위한 영문 프롬프트 사용\n",
        "RAG_prompt = ChatPromptTemplate(\n",
        "    [\n",
        "        (\n",
        "            'system',\n",
        "            '''\n",
        "            Answer the user's Question from the Context.\n",
        "            Keep your answer ground in the facts of the Context.\n",
        "            If the Context doesn't contain the facts to answer, just output '답변할 수 없습니다.'.\n",
        "            Please answer in Korean.\n",
        "            '''\n",
        "        ),\n",
        "        (\n",
        "            'user',\n",
        "            '''\n",
        "            Context: {context}\n",
        "            ---\n",
        "            Question: {question}\n",
        "            '''\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "7pqFPIytxmGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "벡터 데이터베이스 만들기  \n",
        "오픈 임베딩을 hugging face에서 불러와 저장"
      ],
      "metadata": {
        "id": "Xdmt-Oy45Nkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 가장 많이 사용하는 좋은 성능의 임베딩 모델은 BAAI/bge-m3 2~3GB\n",
        "# 이번 실습에서는, Multilingual 모델 중 작은 모델인 intfloat/multilingual-e5-small 사용\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_name = 'intfloat/multilingual-e5-small'\n",
        "emb_model = SentenceTransformer(model_name, device='cpu')  # CPU 설정으로 모델 불러오기\n",
        "\n",
        "emb_model.save('./e5_small')  # 로컬 폴더에 모델 저장\n",
        "\n",
        "del emb_model\n",
        "\n",
        "import gc\n",
        "gc.collect()  # 메모리 정리"
      ],
      "metadata": {
        "id": "HS_o1cwSzsRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "오프라인에서 저장한 임베딩 모델을 다시 불러온다."
      ],
      "metadata": {
        "id": "UnmtX6Kg6Qix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# 허깅페이스 포맷의 임베딩 모델 불러오기\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name='./e5_small',\n",
        "    model_kwargs={'device': 'cuda'}  # CUDA(GPU) 사용 시 속도도 향상되나 GPU 오버 주의\n",
        ")"
      ],
      "metadata": {
        "id": "Y2UawZzN6OsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "벡터 데이터베이스에 저장할 데이터를 수집  \n",
        "papers.zip 파일을 압축 해제"
      ],
      "metadata": {
        "id": "7JCzVvbQ7FBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('papers.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('.')"
      ],
      "metadata": {
        "id": "TXzwK1Zr7JVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "from glob import glob\n",
        "\n",
        "path = './papers/*.pdf'\n",
        "glob(path)  # glob은 경로의 목록을 모두 가져오는 파이썬 기본 라이브러리"
      ],
      "metadata": {
        "id": "sNicHsQL7cw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyMuPDFLoader는 PDF 파일의 텍스트를 불러온다.  \n",
        "이 때, 데이터의 형식은 Document 형식으로 저장된다."
      ],
      "metadata": {
        "id": "OzVbv8DU8pBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "loader = PyMuPDFLoader('./papers/qwen2_5_paper.pdf')\n",
        "pages = loader.load()\n",
        "pages[0]  # 페이지별 Document Class로 저장"
      ],
      "metadata": {
        "id": "30jn6Xqm8oal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "전체 파일에 대해 모두 저장  \n",
        "페이지별 저장된 문서에 대해 청킹 따로하지 않음"
      ],
      "metadata": {
        "id": "OHFhctpq9wYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 PDF 파일을 glob으로 찾음\n",
        "pdf_files = glob.glob(\"./papers/*.pdf\")\n",
        "\n",
        "# 각 PDF 파일에서 페이지별로 내용을 불러와 하나로 합침\n",
        "pages = []\n",
        "\n",
        "for i, path_paper in enumerate(pdf_files):\n",
        "  loader = PyMuPDFLoader(path_paper)\n",
        "  pages += loader.load()\n",
        "\n",
        "print('Total Pages: ', len(pages))\n",
        "pages[2]"
      ],
      "metadata": {
        "id": "ygdZhp-B-E52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "무작위 폴더명을 생성하고 데이터베이스 저장"
      ],
      "metadata": {
        "id": "6MU0t137-rpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU 작업이므로 오래 걸릴 수 있음\n",
        "import uuid\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# 랜덤 폴더명 생성\n",
        "random_dir = f\"./RAG_db_{str(uuid.uuid4())[:8]}\"\n",
        "print(random_dir)\n",
        "\n",
        "db = Chroma.from_documents(\n",
        "    documents=pages,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=random_dir,  # 저장할 디렉토리 위치, 생략하면 메모리 저장\n",
        "    collection_metadata={'hnsw:space':'l2'}  # l2 distance 검색\n",
        "  )\n",
        "\n",
        "retriever = db.as_retriever(search_kwargs={'k': 2})  # Top 2 검색을 수행하는 retriever"
      ],
      "metadata": {
        "id": "0U5TqNvi-joY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_prompt.pretty_print()"
      ],
      "metadata": {
        "id": "e5gmUWGO_35h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "질문이 한국어보다는 영어면 더 잘 동작할 것이므로 영문 변환 체인을 구성"
      ],
      "metadata": {
        "id": "DdEpFH3UDIaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translate_prompt = ChatPromptTemplate(\n",
        "    [\n",
        "        (\n",
        "            'system',\n",
        "            'Trnaslate the following Question to English.'\n",
        "        ),\n",
        "        (\n",
        "            'user',\n",
        "            'Question: {question}'\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "translate_chain = translate_prompt | chat_model | StrOutputParser()"
      ],
      "metadata": {
        "id": "DFsZ4HHYDN_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "translate_chain을 연결하여 전체 체인을 구성"
      ],
      "metadata": {
        "id": "ZN4DkPZ7EVG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "  return \"\\n---\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "# retriever: question을 받아와서 context 검색하고 document 반환\n",
        "# format_docs: 리스트로 document 형태를 받아 텍스트로 변환\n",
        "# RunnablePassthrough(): 체인의 입력을 그대로 저장\n",
        "rag_chain = (\n",
        "    {\"context\": translate_chain | retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | RAG_prompt\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "b01IluIvETPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke(\"Qwen 2.5 Technical Report의 저자는 누구인가요?\")  # 답변 18초"
      ],
      "metadata": {
        "id": "FROMmEeQFBlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# context 증강 확인하기\n",
        "\n",
        "rag_chain_from_docs = (\n",
        "    RAG_prompt\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain_with_source = RunnableParallel(\n",
        "  {\"context\": translate_chain | retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        ").assign(answer=rag_chain_from_docs)\n",
        "\n",
        "rag_chain_with_source.invoke(\"e5의 저자는 누구인가요?\")"
      ],
      "metadata": {
        "id": "8j6Km4xBFk4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain_with_source.invoke(\"Qwen 2.5는 어떤 모델인가요?\")"
      ],
      "metadata": {
        "id": "bdPSbgwoG6kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "만약 구글 코랩이 아닌 PC 환경에서 작업을 수행하거나 모델을 API 형태로 서빙하여 구현하고 싶으면 Ollama(https://ollama.com/)을 사용하면 쉬운 구현 가능"
      ],
      "metadata": {
        "id": "32iPLy0UHiQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Groq, Sambanova 등의 무료 API를 사용해 대형 모델 테스트 가능"
      ],
      "metadata": {
        "id": "sOP7D3BYIdag"
      }
    }
  ]
}